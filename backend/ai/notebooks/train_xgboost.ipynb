{"cells":[{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import roc_auc_score, accuracy_score\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","from xgboost import XGBClassifier"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# Data from https://huggingface.co/datasets/aadityaubhat/GPT-wiki-intro, applied this func to transform data\n","def make_into_dataset(df):\n","    mid = int(len(df) / 2)\n","    x  = df.iloc[:mid, :]['wiki_intro'].to_numpy()\n","    y = df.iloc[mid:, :]['generated_text'].to_numpy()\n","\n","    new_df = {\"text\" : np.concatenate([x,y]), \"ai_generated\": np.concatenate([np.zeros(mid, dtype=int), np.ones(mid, dtype=int)])}\n","    new_df = pd.DataFrame(new_df).sample(frac=1).reset_index(drop=True)\n","    return new_df\n"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>ai_generated</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Dr. Foss Westcott (23 October 186319 October 1...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>The Blacksburg Electronic Village or BEV was c...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>a retired French footballer who played as a m...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Japanese-language dictionary. It is based on ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>drama film, produced and directed by K. Balac...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>149995</th>\n","      <td>am Main – January 22, 1928 in Frankfurt am Ma...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>149996</th>\n","      <td>Shawnae Marie Dixon is an American professiona...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>149997</th>\n","      <td>an American professional basketball executive...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>149998</th>\n","      <td>Düsseldorf Baskets was a professional basketba...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>149999</th>\n","      <td>In linguistics, givenness is a phenomenon in w...</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>150000 rows × 2 columns</p>\n","</div>"],"text/plain":["                                                     text  ai_generated\n","0       Dr. Foss Westcott (23 October 186319 October 1...             0\n","1       The Blacksburg Electronic Village or BEV was c...             0\n","2        a retired French footballer who played as a m...             1\n","3        Japanese-language dictionary. It is based on ...             1\n","4        drama film, produced and directed by K. Balac...             1\n","...                                                   ...           ...\n","149995   am Main – January 22, 1928 in Frankfurt am Ma...             1\n","149996  Shawnae Marie Dixon is an American professiona...             0\n","149997   an American professional basketball executive...             1\n","149998  Düsseldorf Baskets was a professional basketba...             0\n","149999  In linguistics, givenness is a phenomenon in w...             0\n","\n","[150000 rows x 2 columns]"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["df = pd.read_csv('../dataset/llm-classification-data.csv')\n","df"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>ai_generated</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Entwistle is an English surname. Notable peopl...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>for buildings and structures in the UK. They ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Henrietta Hutton (née Cooke) (1939–1963) was a...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>in 2001 by the advertising agency WPP, then k...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Bartolomeu Perestrello (, in Italian Bartolome...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>3995</th>\n","      <td>Astley Abbotts is a village and civil parish i...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3996</th>\n","      <td>Michael Brandon Lake is an American Christian ...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3997</th>\n","      <td>Krishna Kumar Goyal (1932/1933 - 21 April 2013...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3998</th>\n","      <td>is the Founder and President of the David Bri...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3999</th>\n","      <td>American economist who is the Robert P. Balle...</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>4000 rows × 2 columns</p>\n","</div>"],"text/plain":["                                                   text  ai_generated\n","0     Entwistle is an English surname. Notable peopl...             0\n","1      for buildings and structures in the UK. They ...             1\n","2     Henrietta Hutton (née Cooke) (1939–1963) was a...             0\n","3      in 2001 by the advertising agency WPP, then k...             1\n","4     Bartolomeu Perestrello (, in Italian Bartolome...             0\n","...                                                 ...           ...\n","3995  Astley Abbotts is a village and civil parish i...             0\n","3996  Michael Brandon Lake is an American Christian ...             0\n","3997  Krishna Kumar Goyal (1932/1933 - 21 April 2013...             0\n","3998   is the Founder and President of the David Bri...             1\n","3999   American economist who is the Robert P. Balle...             1\n","\n","[4000 rows x 2 columns]"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["def get_subset(df, x):\n","    class_0_data = df[df[\"ai_generated\"] == 0]\n","    class_1_data = df[df[\"ai_generated\"] == 1]\n","\n","    min_samples = min(len(class_0_data), len(class_1_data), x)\n","\n","    balanced_data = pd.concat([class_0_data.sample(min_samples), class_1_data.sample(min_samples)])\n","    balanced_data = balanced_data.sample(frac=1).reset_index(drop=True)\n","\n","    return balanced_data\n","\n","df = get_subset(df, 2000)\n","df"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":[" 60%|██████    | 2418/4000 [00:00<00:00, 6096.59it/s]/var/folders/59/q2yb2fpj1h36gmjgd793x7_w0000gn/T/ipykernel_28658/3835380447.py:21: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n","  soup = BeautifulSoup(text, 'lxml') #Removes HTML tags\n","100%|██████████| 4000/4000 [00:00<00:00, 6123.88it/s]\n"]}],"source":["import re\n","from bs4 import BeautifulSoup\n","from tqdm import tqdm\n","from xgboost import train\n","\n","\n","def text_cleaning(text):\n","    '''\n","    Cleans text into a basic form for NLP. Operations include the following:-\n","    1. Remove special charecters like &, #, etc\n","    2. Removes extra spaces\n","    3. Removes embedded URL links\n","    4. Removes HTML tags\n","    5. Removes emojis\n","    \n","    text - Text piece to be cleaned.\n","    '''\n","    template = re.compile(r'https?://\\S+|www\\.\\S+') #Removes website links\n","    text = template.sub(r'', text)\n","    \n","    soup = BeautifulSoup(text, 'lxml') #Removes HTML tags\n","    only_text = soup.get_text()\n","    text = only_text\n","    \n","    emoji_pattern = re.compile(\"[\"\n","                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n","                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n","                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n","                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n","                               u\"\\U00002702-\\U000027B0\"\n","                               u\"\\U000024C2-\\U0001F251\"\n","                               \"]+\", flags=re.UNICODE)\n","    text = emoji_pattern.sub(r'', text)\n","    \n","    text = re.sub(r\"[^a-zA-Z\\d]\", \" \", text) #Remove special Charecters\n","    text = re.sub(' +', ' ', text) #Remove Extra Spaces\n","    text = text.strip() # remove spaces at the beginning and at the end of string\n","\n","    return text\n","tqdm.pandas()\n","df['text'] = df['text'].progress_apply(text_cleaning)"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"ename":"AttributeError","evalue":"'csr_matrix' object has no attribute 'lower'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[22], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Compute TFIDF and set X as an array\u001b[39;00m\n\u001b[1;32m      2\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer()\n\u001b[0;32m----> 3\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m X\u001b[38;5;241m.\u001b[39mtoarray()\n","File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:2126\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[1;32m   2120\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[1;32m   2121\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[1;32m   2122\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[1;32m   2123\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[1;32m   2124\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[1;32m   2125\u001b[0m )\n\u001b[0;32m-> 2126\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2127\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[1;32m   2128\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[1;32m   2129\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n","File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:1383\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1375\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1376\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1377\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1378\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1379\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1380\u001b[0m             )\n\u001b[1;32m   1381\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1383\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1385\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[1;32m   1386\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n","File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:1270\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1268\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[1;32m   1269\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m-> 1270\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1271\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1272\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n","File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:110\u001b[0m, in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m preprocessor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 110\u001b[0m         doc \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    112\u001b[0m         doc \u001b[38;5;241m=\u001b[39m tokenizer(doc)\n","File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:68\u001b[0m, in \u001b[0;36m_preprocess\u001b[0;34m(doc, accent_function, lower)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Chain together an optional series of text preprocessing steps to\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03mapply to a document.\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    preprocessed string\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lower:\n\u001b[0;32m---> 68\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m()\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accent_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m     doc \u001b[38;5;241m=\u001b[39m accent_function(doc)\n","\u001b[0;31mAttributeError\u001b[0m: 'csr_matrix' object has no attribute 'lower'"]}],"source":["vec = TfidfVectorizer(min_df= 20,\n","                      max_df=0.05,\n","                      analyzer = 'char_wb',\n","                      ngram_range = (3,5),\n","                      max_features = 1000)\n","vec.fit(train['text'])\n","tfidf_matrix = vec.transform(train['text'])\n","tfidf_matrix"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"ename":"ValueError","evalue":"index can't contain negative values","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m max_corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10000\u001b[39m\n\u001b[0;32m----> 2\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_corpus\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mconstant\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Split the data into training and testing sets\u001b[39;00m\n\u001b[1;32m      5\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, labels, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n","File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mpad\u001b[0;34m(*args, **kwargs)\u001b[0m\n","File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/numpy/lib/arraypad.py:744\u001b[0m, in \u001b[0;36mpad\u001b[0;34m(array, pad_width, mode, **kwargs)\u001b[0m\n\u001b[1;32m    741\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`pad_width` must be of integral type.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;66;03m# Broadcast to shape (array.ndim, 2)\u001b[39;00m\n\u001b[0;32m--> 744\u001b[0m pad_width \u001b[38;5;241m=\u001b[39m \u001b[43m_as_pairs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpad_width\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mndim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(mode):\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;66;03m# Old behavior: Use user-supplied function with np.apply_along_axis\u001b[39;00m\n\u001b[1;32m    748\u001b[0m     function \u001b[38;5;241m=\u001b[39m mode\n","File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/numpy/lib/arraypad.py:514\u001b[0m, in \u001b[0;36m_as_pairs\u001b[0;34m(x, ndim, as_index)\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m ((x[\u001b[38;5;241m0\u001b[39m], x[\u001b[38;5;241m1\u001b[39m]),) \u001b[38;5;241m*\u001b[39m ndim\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m as_index \u001b[38;5;129;01mand\u001b[39;00m x\u001b[38;5;241m.\u001b[39mmin() \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 514\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt contain negative values\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    516\u001b[0m \u001b[38;5;66;03m# Converting the array with `tolist` seems to improve performance\u001b[39;00m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;66;03m# when iterating and indexing the result (see usage in `pad`)\u001b[39;00m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mbroadcast_to(x, (ndim, \u001b[38;5;241m2\u001b[39m))\u001b[38;5;241m.\u001b[39mtolist()\n","\u001b[0;31mValueError\u001b[0m: index can't contain negative values"]}],"source":["max_corpus = 10000\n","X = np.pad(X, ((0, 0), (0, max_corpus - X.shape[1])), 'constant')\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define the hyperparameters grid for XGBoost\n","param_grid = {\n","    'n_estimators': [100, 200],\n","    'learning_rate': [0.1, 0.2],\n","    'max_depth': [3, 5]\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Initialize the XGBoost classifier\n","xgb = XGBClassifier()\n","\n","# Perform Grid Search with cross-validation\n","grid_search = GridSearchCV(xgb, param_grid, cv=3, scoring='accuracy')\n","grid_search.fit(X_train, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Get the best parameters and model\n","best_params = grid_search.best_params_\n","best_model = grid_search.best_estimator_\n","\n","# Predict on the test set using the best model\n","predictions = best_model.predict(X_test)\n","predictions"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Evaluate the best model\n","accuracy = accuracy_score(y_test, predictions)\n","roc = roc_auc_score(y_test, predictions)\n","print(\"Best Parameters:\", best_params)\n","print(\"Accuracy:\", accuracy)\n","print(\"ROC AUC:\", roc)"]}],"metadata":{"kernelspec":{"display_name":"navya-streamlit-kr_rObhs","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"}},"nbformat":4,"nbformat_minor":2}
